import streamlit as st
import cv2
import numpy as np
from PIL import Image
import pickle
from scipy.spatial.distance import cosine
from sklearn.cluster import MiniBatchKMeans
from sklearn.preprocessing import normalize

class BOVWSearcher:
    def __init__(self, database_path):
        print("ƒêang load BOVW database...")
        with open(database_path, 'rb') as f:
            data = pickle.load(f)
            self.database = data['database']
            self.vocabulary = data['vocabulary']
            self.n_clusters = data['n_clusters']
            self.idf_weights = data.get('idf_weights', np.ones(self.n_clusters))
        
        self.sift = cv2.SIFT_create()
        self.kmeans = MiniBatchKMeans(
            n_clusters=self.n_clusters,
            random_state=42,
            batch_size=1000,
            n_jobs=1,
            verbose=0
        )
        self.kmeans.cluster_centers_ = self.vocabulary
        self.kmeans.fit(self.vocabulary)
        
        print(f"ƒê√£ load database v·ªõi {len(self.database)} ·∫£nh")

    def process_query_image(self, image):
        try:
            if len(image.shape) == 3:
                gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            else:
                gray = image
                
            keypoints, descriptors = self.sift.detectAndCompute(gray, None)
            
            if descriptors is None:
                return None
                
            # T·∫°o BOVW histogram t∆∞∆°ng t·ª± nh∆∞ trong training
            descriptors = normalize(descriptors, norm='l2', axis=1)
            distances = self.kmeans.transform(descriptors)
            
            sigma = np.mean(distances) / 2
            weights = np.exp(-distances / (2 * sigma**2))
            weights = weights / weights.sum(axis=1, keepdims=True)
            
            histogram = np.zeros(self.n_clusters)
            
            if keypoints:
                kp_weights = np.array([kp.size * kp.response for kp in keypoints])
                kp_weights = kp_weights / np.sum(kp_weights)
                for i in range(len(descriptors)):
                    histogram += weights[i] * kp_weights[i]
            else:
                histogram = weights.sum(axis=0)
            
            histogram *= self.idf_weights
            histogram = normalize(histogram.reshape(1, -1), norm='l2')[0]
            
            return histogram
            
        except Exception as e:
            st.error(f"L·ªói khi x·ª≠ l√Ω ·∫£nh: {str(e)}")
            return None

    def search_image(self, query_image, top_k=5):
        query_features = self.process_query_image(query_image)
        if query_features is None:
            return []
            
        results = []
        for image_name, features in self.database.items():
            similarity = 1 - cosine(query_features, features['histogram'])
            results.append({
                'image_name': image_name,
                'score': similarity,
                'image': features['image']
            })
            
        results.sort(key=lambda x: x['score'], reverse=True)
        return results[:top_k]

def main():
    st.set_page_config(page_title="Image Search Demo", layout="wide")
    
    # Ph·∫ßn 1: Gi·ªõi thi·ªáu Dataset
    st.title("Demo H·ªá th·ªëng T√¨m ki·∫øm ·∫¢nh")
    st.header("1. Gi·ªõi thi·ªáu Dataset")
    st.write("""
    Dataset g·ªìm 5000 ·∫£nh ƒëa d·∫°ng ƒë∆∞·ª£c thu th·∫≠p t·ª´ COCO dataset, bao g·ªìm:
    - C√°c ƒë·ªëi t∆∞·ª£ng th∆∞·ªùng g·∫∑p trong cu·ªôc s·ªëng h√†ng ng√†y
    - Phong c·∫£nh thi√™n nhi√™n
    - Con ng∆∞·ªùi v√† ƒë·ªông v·∫≠t
    - ƒê·ªì v·∫≠t, ph∆∞∆°ng ti·ªán giao th√¥ng
    - C√°c ho·∫°t ƒë·ªông v√† s·ª± ki·ªán
    
    M·ªói ·∫£nh c√≥ k√≠ch th∆∞·ªõc v√† n·ªôi dung kh√°c nhau, gi√∫p ƒë√°nh gi√° hi·ªáu qu·∫£ c·ªßa h·ªá th·ªëng t√¨m ki·∫øm trong nhi·ªÅu t√¨nh hu·ªëng kh√°c nhau.
    """)
    
    # Danh s√°ch t√™n c√°c ·∫£nh m·∫´u
    sample_images = [
        "DTS/000000001675.jpg", "DTS/000000001761.jpg", "DTS/000000001818.jpg", "DTS/000000001993.jpg", "DTS/000000002006.jpg",
        "DTS/000000002149.jpg", "DTS/000000002153.jpg", "DTS/000000002157.jpg", "DTS/000000002261.jpg", "DTS/000000002299.jpg"
    ]
    
    # T·∫°o 2 h√†ng, m·ªói h√†ng 5 c·ªôt ƒë·ªÉ hi·ªÉn th·ªã ·∫£nh m·∫´u
    for row in range(2):
        cols = st.columns(5)
        for idx, col in enumerate(cols):
            image_index = row * 5 + idx
            with col:
                try:
                    st.image(sample_images[image_index], 
                            caption=f"·∫¢nh m·∫´u {image_index + 1}",
                            use_column_width=True)
                except Exception as e:
                    st.error(f"Kh√¥ng th·ªÉ load ·∫£nh {sample_images[image_index]}")
    
    # Ph·∫ßn 2: Gi·ªõi thi·ªáu Quy tr√¨nh
    st.header("2. Quy tr√¨nh x·ª≠ l√Ω BOVW")
    
    # SIFT Feature Extraction
    st.subheader("2.1. Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng SIFT")
    st.write("""
    - S·ª≠ d·ª•ng SIFT ƒë·ªÉ tr√≠ch xu·∫•t keypoints v√† descriptors t·ª´ ·∫£nh
    - M·ªói keypoint ch·ª©a th√¥ng tin v·ªÅ v·ªã tr√≠, scale, v√† orientation
    - M·ªói descriptor l√† vector 128 chi·ªÅu
    """)
    st.image("SIFT-feature-extraction-algorithm-process.png", 
             caption="SIFT keypoints v√† descriptors", 
             use_column_width=True)
    
    # Visual Vocabulary Construction
    st.subheader("2.2. X√¢y d·ª±ng Vocabulary")
    st.write("""
    - Thu th·∫≠p t·∫•t c·∫£ SIFT descriptors t·ª´ dataset
    - S·ª≠ d·ª•ng K-means clustering ƒë·ªÉ t·∫°o visual words
    - S·ªë l∆∞·ª£ng clusters = 1000 (c√≥ th·ªÉ ƒëi·ªÅu ch·ªânh)
    """)
    st.image("The-features-extraction-system-using-bag-of-visual-words-BoVW.png", 
             caption="K-means clustering visual words", 
             use_column_width=True)
    
    st.subheader("2.3. C√°c k·ªπ thu·∫≠t trong BOVW Histogram")
    
    st.write("""
    1. **Soft Assignment (G√°n m·ªÅm)**
    - Thay v√¨ g√°n c·ª©ng m·ªôt descriptor v√†o m·ªôt visual word
    - Cho ph√©p m·ªôt descriptor ƒë√≥ng g√≥p v√†o nhi·ªÅu visual words v·ªõi tr·ªçng s·ªë kh√°c nhau
    - Gi√∫p gi·∫£m l·ªói v√† tƒÉng ƒë·ªô ch√≠nh x√°c khi so s√°nh ·∫£nh
    
    2. **Keypoint Weighting (Tr·ªçng s·ªë ƒëi·ªÉm ƒë·∫∑c tr∆∞ng)**
    - D·ª±a tr√™n size (k√≠ch th∆∞·ªõc) v√† response (ƒë·ªô t∆∞∆°ng ph·∫£n) c·ªßa keypoint
    - ƒêi·ªÉm ƒë·∫∑c tr∆∞ng quan tr·ªçng (to v√† n·ªïi b·∫≠t) ƒë∆∞·ª£c cho tr·ªçng s·ªë cao h∆°n
    - Gi√∫p t·∫≠p trung v√†o c√°c ƒë·∫∑c ƒëi·ªÉm quan tr·ªçng c·ªßa ·∫£nh
    
    3. **IDF Weighting (Tr·ªçng s·ªë IDF)**
    - ƒê√°nh gi√° m·ª©c ƒë·ªô quan tr·ªçng c·ªßa visual words trong to√†n b·ªô dataset
    - Visual words hi·∫øm (xu·∫•t hi·ªán √≠t) ƒë∆∞·ª£c coi l√† quan tr·ªçng h∆°n
    - Gi√∫p ph√¢n bi·ªát t·ªët h∆°n c√°c ƒë·∫∑c tr∆∞ng ƒë·ªôc ƒë√°o c·ªßa ·∫£nh
    
    4. **Normalize Histogram (Chu·∫©n h√≥a)**
    - Chu·∫©n h√≥a histogram ƒë·ªÉ c√≥ t·ªïng b·∫±ng 1
    - Gi√∫p so s√°nh c√¥ng b·∫±ng gi·ªØa c√°c ·∫£nh c√≥ s·ªë l∆∞·ª£ng keypoints kh√°c nhau
    - C·∫£i thi·ªán ƒë·ªô ch√≠nh x√°c khi t√≠nh to√°n ƒë·ªô t∆∞∆°ng ƒë·ªìng
    """)
    
    # Th√™m m·ªôt v√≠ d·ª• t∆∞∆°ng t√°c
    st.write("#### 2.3.5. Th·ª≠ nghi·ªám tr·ª±c quan")
    if st.checkbox("Xem v√≠ d·ª• v·ªÅ qu√° tr√¨nh x·ª≠ l√Ω"):
        st.write("""
        üéØ **V√≠ d·ª• th·ª±c t·∫ø:**
        1. Khi b·∫°n t√¨m ki·∫øm m·ªôt chi·∫øc iPhone trong ·∫£nh:
           - Soft Assignment: Logo Apple c√≥ th·ªÉ gi·ªëng 90% logo Apple th·∫≠t, 10% gi·ªëng h√¨nh tr√≤n th√¥ng th∆∞·ªùng
           - Keypoint: Logo Apple s·∫Ω c√≥ tr·ªçng s·ªë cao v√¨ n√≥ n·ªïi b·∫≠t v√† c√≥ k√≠ch th∆∞·ªõc ƒë√°ng k·ªÉ
           - IDF: Logo Apple c√≥ IDF cao v√¨ kh√¥ng ph·∫£i ·∫£nh n√†o c≈©ng c√≥ logo n√†y
           - Normalize: D√π ·∫£nh to hay nh·ªè, ch√∫ng ta v·∫´n nh·∫≠n ra ƒë√≥ l√† iPhone
        """)
    
    # Ph·∫ßn 3: Instance Search
    st.header("3. Instance Search")
    
    # Th√™m sidebar cho c√°c t√πy ch·ªçn
    st.sidebar.title("T√πy ch·ªçn t√¨m ki·∫øm")
    top_k = st.sidebar.slider("S·ªë l∆∞·ª£ng k·∫øt qu·∫£", min_value=1, max_value=20, value=5)
    
    # Th√™m th√¥ng tin v·ªÅ ·ª©ng d·ª•ng trong sidebar
    st.sidebar.markdown("---")
    st.sidebar.subheader("V·ªÅ ·ª©ng d·ª•ng")
    st.sidebar.write("""
    ·ª®ng d·ª•ng s·ª≠ d·ª•ng:
    - SIFT ƒë·ªÉ tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng
    - BOVW ƒë·ªÉ bi·ªÉu di·ªÖn ·∫£nh
    - Cosine similarity ƒë·ªÉ so s√°nh
    """)
    
    # Upload ·∫£nh
    uploaded_file = st.file_uploader("Ch·ªçn ·∫£nh ƒë·ªÉ t√¨m ki·∫øm...", type=['jpg', 'jpeg', 'png'])
    
    if uploaded_file is not None:
        # Hi·ªÉn th·ªã ·∫£nh query v√† k·∫øt qu·∫£ trong 2 c·ªôt
        col1, col2 = st.columns([1, 3])
        
        with col1:
            st.subheader("·∫¢nh Query")
            query_image = Image.open(uploaded_file)
            st.image(query_image, use_column_width=True)
        
        # X·ª≠ l√Ω ·∫£nh v√† t√¨m ki·∫øm
        try:
            query_array = np.array(query_image)
            if len(query_array.shape) == 2:
                query_array = cv2.cvtColor(query_array, cv2.COLOR_GRAY2BGR)
            elif query_array.shape[2] == 4:
                query_array = cv2.cvtColor(query_array, cv2.COLOR_RGBA2BGR)
            
            with st.spinner('ƒêang t√¨m ki·∫øm...'):
                searcher = BOVWSearcher("bovw_database.pkl")
                results = searcher.search_image(query_array, top_k=top_k)
            
            # Hi·ªÉn th·ªã k·∫øt qu·∫£
            with col2:
                st.subheader("K·∫øt qu·∫£ t√¨m ki·∫øm")
                if results:
                    cols = st.columns(3)
                    for idx, result in enumerate(results):
                        col_idx = idx % 3
                        with cols[col_idx]:
                            st.image(result['image'],
                                   caption=f"Score: {result['score']:.3f}\n{result['image_name']}",
                                   use_column_width=True)
                else:
                    st.warning("Kh√¥ng t√¨m th·∫•y ·∫£nh t∆∞∆°ng t·ª±!")
                    
        except Exception as e:
            st.error(f"C√≥ l·ªói x·∫£y ra: {str(e)}")

if __name__ == "__main__":
    main()